{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "#import row data\n",
    "row_data = np.loadtxt('sarcos_inv.csv',delimiter = ',')\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(row_data)\n",
    "X = row_data[:,:21]\n",
    "Y = row_data[:,21:]\n",
    "\n",
    "# normalize_x\n",
    "X = (X - np.mean(X)) / np.std(X)\n",
    "\n",
    "#split data\n",
    "split_size = [0.6,0.2,0.2]\n",
    "index_train = int(X.shape[0] * split_size[0])\n",
    "index_cv = index_train + int(X.shape[0] * split_size[1])\n",
    "index_test = index_cv + int(X.shape[0] * split_size[2])\n",
    "X_train = X[:index_train, :]\n",
    "Y_train = Y[:index_train, :]\n",
    "X_test = X[index_cv:index_test, :]\n",
    "Y_test = Y[index_cv:index_test, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    def __init__(self, is_leaf, mean, std, feature, split, left, right):\n",
    "        \"\"\"\n",
    "        :param is_leaf: Whether or not it is a leaf node\n",
    "        :param mean: The current mean of all the examples contained in the node\n",
    "        :param std: The current standard deviation of all the examples contained in the node\n",
    "        :param feature: The feature by which the split is performed\n",
    "        :param split: The actual threshold value for the split\n",
    "        :param left: Left child node\n",
    "        :param right: Right child node\n",
    "        \"\"\"\n",
    "        self.is_leaf = is_leaf\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.feature = feature\n",
    "        self.split = split\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, X_train, Y_train, min_nodes):\n",
    "        \"\"\"\n",
    "        :param X_train: Training set\n",
    "        :param Y_train: Output values for the training set\n",
    "        :param min_nodes: If a node contains equal or less than 'min_nodes',\n",
    "                            then the recursion is terminated and a leaf node is created\n",
    "        \"\"\"\n",
    "        self.root = self._create_tree(X_train, Y_train, min_nodes)\n",
    "\n",
    "#     def predict(self, x):\n",
    "#         \"\"\"\n",
    "#         Predicts the output value of the input vector.\n",
    "#         :param x: Input vector\n",
    "#         :return: Mean of the leaf node, standard deviation of the leaf node\n",
    "#         \"\"\"\n",
    "#         return self._predict(self.root, x)\n",
    "\n",
    "#     def predict_all(self, X):\n",
    "#         \"\"\"\n",
    "#         Predicts the output values of an entire set of input vectors.\n",
    "#         :param X: Set of input vectors\n",
    "#         :return: Mean of the leaf nodes, standard deviation of the leaf nodes\n",
    "#         \"\"\"\n",
    "#         means = np.zeros((X.shape[0], 1))\n",
    "#         stds = np.zeros((X.shape[0], 1))\n",
    "#         for i in range(X.shape[0]):\n",
    "#             mean, std = self._predict(self.root, X[i])\n",
    "#             means[i] = mean\n",
    "#             stds[i] = std\n",
    "#         return means, stds\n",
    "\n",
    "#     def _predict(self, node, x):\n",
    "#         \"\"\"\n",
    "#         Helper function for the predict function.\n",
    "#         :param node: Current node in the tree\n",
    "#         :param x: Input vector for which we predict the output value\n",
    "#         :return: Mean of the leaf node, standard deviation of the leaf node\n",
    "#         \"\"\"\n",
    "#         if node.is_leaf:\n",
    "#             return node.mean, node.std\n",
    "#         if x[node.feature] <= node.split:\n",
    "#             return self._predict(node.left, x)\n",
    "#         else:\n",
    "#             return self._predict(node.right, x)\n",
    "\n",
    "    def _create_tree(self, X, Y, min_nodes):\n",
    "        \"\"\"\n",
    "        Recursively creates a decision tree.\n",
    "        :param X: Current set of training examples\n",
    "        :param Y: Output values for the training examples\n",
    "        :param min_nodes: If a node contains equal or less than 'min_nodes', then\n",
    "                            the recursion is terminated and a leaf node is created\n",
    "        :return: Eventually returns the root of the decision tree\n",
    "        \"\"\"\n",
    "        if len(X) <= min_nodes:\n",
    "            return Node(is_leaf=True, mean=np.mean(Y), std=np.std(Y), feature=None, split=None, left=None, right=None)\n",
    "        feature, best_split, left_indices, right_indices = DecisionTree._best_split_random(X, Y)\n",
    "        left = self._create_tree(X[left_indices], Y[left_indices], min_nodes)\n",
    "        right = self._create_tree(X[right_indices], Y[right_indices], min_nodes)\n",
    "        return Node(is_leaf=False, mean=np.mean(Y), std=np.std(Y), feature=feature, split=best_split, left=left, right=right)\n",
    "\n",
    "    @staticmethod\n",
    "    def _best_split_random(X, Y):\n",
    "        \"\"\"\n",
    "        Computes the best split for a given set of training examples.\n",
    "        Tries out only a random subset (of size num_features / 2 + 1) of the features.\n",
    "        Tries out only a random subset (of size sqrt(num_values)*2) of all of the existing values for a specific feature.\n",
    "        :param X: Set of training examples\n",
    "        :param Y: Output values of training examples\n",
    "        :return: Feature for split, threshold value for split, indices of left child, indices of right child\n",
    "        \"\"\"\n",
    "        min_sse = np.inf\n",
    "        feature = None\n",
    "        best_split = None\n",
    "        left = None\n",
    "        right = None\n",
    "        random_feature_indices_ = np.random.choice(np.arange(X.shape[1]), size=int(X.shape[1] // 2 + 1), replace=False)\n",
    "        for j in range(len(random_feature_indices_)):\n",
    "            i = random_feature_indices_[j]\n",
    "            num_values = X[:, i].shape[0]\n",
    "            random_values_indices = np.random.choice(np.arange(num_values), size=int(2 * np.sqrt(num_values)), replace=False)\n",
    "            values = X[random_values_indices, i]\n",
    "            for split in values:\n",
    "                left_indices = X[:, i:(i+1)] <= split\n",
    "                left_indices, _ = np.nonzero(left_indices)\n",
    "                right_indices = X[:, i:(i+1)] > split\n",
    "                right_indices, _ = np.nonzero(right_indices)\n",
    "                sse = DecisionTree._sum_of_squared_errors(Y[left_indices], Y[right_indices])\n",
    "                if sse < min_sse:\n",
    "                    feature = i\n",
    "                    best_split = split\n",
    "                    left = left_indices\n",
    "                    right = right_indices\n",
    "                    min_sse = sse\n",
    "        return feature, best_split, left, right\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _best_split(X, Y):\n",
    "#         \"\"\"\n",
    "#         Computes the best split for a given set of training examples.\n",
    "#         Tries out all of the existing values for a specific feature.\n",
    "#         :param X: Set of training examples\n",
    "#         :param Y: Output values of training examples\n",
    "#         :return: Feature for split, threshold value for split, indices of left child, indices of right child\n",
    "#         \"\"\"\n",
    "#         min_sse = np.inf\n",
    "#         feature = None\n",
    "#         best_split = None\n",
    "#         left = None\n",
    "#         right = None\n",
    "#         for i in range(X.shape[1]):\n",
    "#             values = np.unique(X[:, i])\n",
    "#             for split in values:\n",
    "#                 left_indices = X[:, i:(i+1)] <= split\n",
    "#                 left_indices, _ = np.nonzero(left_indices)\n",
    "#                 right_indices = X[:, i:(i+1)] > split\n",
    "#                 right_indices, _ = np.nonzero(right_indices)\n",
    "#                 sse = DecisionTree._sum_of_squared_errors(Y[left_indices], Y[right_indices])\n",
    "#                 if sse < min_sse:\n",
    "#                     feature = i\n",
    "#                     best_split = split\n",
    "#                     left = left_indices\n",
    "#                     right = right_indices\n",
    "#                     min_sse = sse\n",
    "#         return feature, best_split, left, right\n",
    "\n",
    "    @staticmethod\n",
    "    def _sum_of_squared_errors(Y1, Y2):\n",
    "        \"\"\"\n",
    "        Computes the sum of squared errors between two vectors.\n",
    "        :param Y1: First vector\n",
    "        :param Y2: Second vector\n",
    "        :return: Sum of squared errors\n",
    "        \"\"\"\n",
    "        if len(Y1) == 0:\n",
    "            sse1 = 0\n",
    "        else:\n",
    "            sse1 = np.sum((Y1 - np.mean(Y1))**2)\n",
    "        if len(Y2) == 0:\n",
    "            sse2 = 0\n",
    "        else:\n",
    "            sse2 = np.sum((Y2 - np.mean(Y2)) ** 2)\n",
    "        return sse1 + sse2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "\n",
    "    def __init__(self, X_train, Y_train, num_trees, training_set_size, min_nodes):\n",
    "        \"\"\"\n",
    "        :param X_train: Training set\n",
    "        :param Y_train: Output values for the training set\n",
    "        :param num_trees: Number of decision trees\n",
    "        :param training_set_size: Size of the training set that each decision tree is build with\n",
    "        :param min_nodes: If a node contains equal or less than 'min_nodes',\n",
    "                            then the recursion is terminated and a leaf node is created\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.tree_list = []\n",
    "        self._create_forest(num_trees, training_set_size, min_nodes)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the output value of the input vector using\n",
    "        the average over all of the prediction from the decision trees.\n",
    "        :param x: Input vector\n",
    "        :return: Mean of the leaf nodes, standard deviation of the leaf nodes\n",
    "        \"\"\"\n",
    "        mean_all = 0\n",
    "        std_all = 0\n",
    "        for tree in self.tree_list:\n",
    "            mean, std = tree.predict(x)\n",
    "            mean_all = mean_all + mean\n",
    "            std_all = std_all + std\n",
    "        mean_all = mean_all / len(self.tree_list)\n",
    "        std_all = std_all / len(self.tree_list)\n",
    "        return mean_all, std_all\n",
    "\n",
    "    def predict_all(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the output values of an entire set of input vectors\n",
    "        using the average over all of the prediction from the decision trees.\n",
    "        :param X: Set of input vectors\n",
    "        :return: Mean of the leaf nodes, standard deviation of the leaf nodes\n",
    "        \"\"\"\n",
    "        means = np.zeros((X.shape[0], 1))\n",
    "        stds = np.zeros((X.shape[0], 1))\n",
    "        for i in range(X.shape[0]):\n",
    "            mean, std = self.predict(X[i])\n",
    "            means[i] = mean\n",
    "            stds[i] = std\n",
    "        return means, stds\n",
    "\n",
    "    def _create_forest(self, num_trees, training_set_size, min_nodes):\n",
    "        \"\"\"\n",
    "        Creates a random forest.\n",
    "        :param num_trees: Number of decision trees\n",
    "        :param training_set_size: Size of the training set that each decision tree is build with\n",
    "        :param min_nodes: If a node contains equal or less than 'min_nodes',\n",
    "                            then the recursion is terminated and a leaf node is created\n",
    "        \"\"\"\n",
    "        for i in range(num_trees):\n",
    "            tree = self._create_tree(training_set_size, min_nodes)\n",
    "            self.tree_list.append(tree)\n",
    "\n",
    "    def _create_tree(self, training_set_size, min_nodes):\n",
    "        \"\"\"\n",
    "        Creates a decision tree from a random subset of the training examples.\n",
    "        :param training_set_size: Size of the training set that each decision tree is build with\n",
    "        :param min_nodes: If a node contains equal or less than 'min_nodes',\n",
    "                            then the recursion is terminated and a leaf node is created\n",
    "        :return: Decision tree\n",
    "        \"\"\"\n",
    "        random_subset_indices = np.random.choice(np.arange(self.X_train.shape[0]), size=training_set_size, replace=False)\n",
    "        X = self.X_train[random_subset_indices]\n",
    "        Y = self.Y_train[random_subset_indices]\n",
    "        tree = DecisionTree(X, Y, min_nodes=min_nodes)\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarcos_regression_forest():\n",
    "    print('Regression forests, sarcos:')\n",
    "    print('\\tThis will take about 6 minutes, please wait...')\n",
    "    model = RandomForest(X_train, Y_train, num_trees=30, training_set_size=4000, min_nodes=5)\n",
    "    mean, std = model.predict_all(X_test)\n",
    "    rmse = np.sqrt(np.mean(np.square(mean - Y_test)))\n",
    "    print('\\tRMSE: ' + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression forests, sarcos:\n",
      "\tThis will take about 6 minutes, please wait...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DecisionTree' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f7c05c1f0b33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msarcos_regression_forest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-f65d32d00587>\u001b[0m in \u001b[0;36msarcos_regression_forest\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\tThis will take about 6 minutes, please wait...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_trees\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_set_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\tRMSE: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d59d761dd945>\u001b[0m in \u001b[0;36mpredict_all\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mstds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0mmeans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mstds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d59d761dd945>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mstd_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mmean_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_all\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mstd_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstd_all\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DecisionTree' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "sarcos_regression_forest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
