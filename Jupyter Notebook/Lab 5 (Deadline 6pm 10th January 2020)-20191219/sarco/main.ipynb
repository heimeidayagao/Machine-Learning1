{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "        :param path: File path of the data\n",
    "        \"\"\"\n",
    "        data = np.loadtxt(path, delimiter=',')\n",
    "        np.random.seed(1)\n",
    "        np.random.shuffle(data)\n",
    "        self.X = data[:, :21]\n",
    "        self.Y = data[:, 21:]\n",
    "\n",
    "    def get_all_data(self, normalize=False):\n",
    "        \"\"\"\n",
    "        Returns the entire data set.\n",
    "        :param normalize: Whether or not the data will be normalized\n",
    "        :return: Data set, output values\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            return DataLoader._normalize(self.X), self.Y\n",
    "        return self.X, self.Y\n",
    "\n",
    "    def get_data_split(self, split, normalize=False):\n",
    "        \"\"\"\n",
    "        Returns the data according to the specified splits.\n",
    "        The 'split' parameter is a vector with either two or three components that in both cases have to sum to 1.\n",
    "        If the 'split' vector contains two components, the first component specifies the portion of the data set\n",
    "        that will be used for the training set and the second component specifies the portion of the data set that\n",
    "        will be used for the test set.\n",
    "        If the 'split' vector contains three components, the first and last components will be interpreted as described\n",
    "        above and the middle component will be used to specify the portion of the data set that will be used for the\n",
    "        cross validation set.\n",
    "        :param split: A vector containing either 2 or 3 values.\n",
    "        :param normalize: Whether or not the data will be normalized\n",
    "        :return: Training set, output values for training set, cross validation set, output values for cross validation\n",
    "                    set, test set, output values for cross validation set\n",
    "        \"\"\"\n",
    "        assert np.sum(split) == 1\n",
    "        X = self.X\n",
    "        if normalize:\n",
    "            X = DataLoader._normalize(self.X)\n",
    "        if len(split) == 2:\n",
    "            idx_train = int(np.round(self.X.shape[0] * split[0]))\n",
    "            X_train = X[:idx_train, :]\n",
    "            Y_train = self.Y[:idx_train, :]\n",
    "            X_test = X[idx_train:, :]\n",
    "            Y_test = self.Y[idx_train:, :]\n",
    "            return X_train, Y_train, X_test, Y_test\n",
    "        if len(split) == 3:\n",
    "            idx_train = int(np.round(self.X.shape[0] * split[0]))\n",
    "            idx_cv = idx_train + int(np.round(self.X.shape[0] * split[1]))\n",
    "            idx_test = idx_cv + int(np.round(self.X.shape[0] * split[2]))\n",
    "            X_train = X[:idx_train, :]\n",
    "            Y_train = self.Y[:idx_train, :]\n",
    "            X_cv = X[idx_train:idx_cv, :]\n",
    "            Y_cv = self.Y[idx_train:idx_cv, :]\n",
    "            X_test = X[idx_cv:idx_test, :]\n",
    "            Y_test = self.Y[idx_cv:idx_test, :]\n",
    "            return X_train, Y_train, X_cv, Y_cv, X_test, Y_test\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize(X):\n",
    "        \"\"\"\n",
    "        Normalizes the data.\n",
    "        :param X: Data\n",
    "        :return: Normalized data\n",
    "        \"\"\"\n",
    "        return (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest neighbour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestNeighbors:\n",
    "\n",
    "    def __init__(self, X_train, Y_train):\n",
    "        \"\"\"\n",
    "        :param X_train: training set\n",
    "        :param Y_train: outputs of the training set\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "\n",
    "    def predict(self, x, k):\n",
    "        \"\"\"\n",
    "        Predicts the output value of the provided example.\n",
    "        :param x: Example input vector\n",
    "        :param k: The number of neighbors used for computing the prediction\n",
    "        :return: Prediction made based on the k neighbors\n",
    "        \"\"\"\n",
    "        dist = np.linalg.norm(self.X_train - x, axis=1)\n",
    "        idx = np.argpartition(dist, k)[:k]\n",
    "        weights = 1 / dist[idx]**2\n",
    "        return np.sum(np.squeeze(self.Y_train[idx]) * np.squeeze(weights)) / np.sum(weights)\n",
    "        #return np.sum(np.squeeze(self.Y_train[idx])) / k\n",
    "\n",
    "    def test(self, X_test, Y_test, k):\n",
    "        \"\"\"\n",
    "        Tests the performance of the algorithm by means of the provided test set.\n",
    "        :param X_test: Test set\n",
    "        :param Y_test: Output values for the test set\n",
    "        :param k: The number of neighbors used for computing the prediction\n",
    "        :return: RMSE of the test set, predictions made on the test set\n",
    "        \"\"\"\n",
    "        X_pred = np.zeros(Y_test.shape)\n",
    "        for i in range(X_pred.shape[0]):\n",
    "            X_pred[i][0] = self.predict(X_test[i], k)\n",
    "        rmse = np.sqrt(np.mean(np.square(X_pred - Y_test)))\n",
    "        return rmse, X_pred\n",
    "\n",
    "    def get_best_k(self, X, Y, limit):\n",
    "        \"\"\"\n",
    "        Tries out values for k in the range from 1 to 'limit', returns the k that yields the lowest RMSE.\n",
    "        :param X: Cross validation set\n",
    "        :param Y: Output values for the cross validation set\n",
    "        :param limit: Range limit for k\n",
    "        :return: Optimal k for the given range\n",
    "        \"\"\"\n",
    "        best_k = -1\n",
    "        best_rmse = np.inf\n",
    "        for k in range(1, limit + 1):\n",
    "            rmse, _ = self.test(X, Y, k)\n",
    "            if rmse < best_rmse:\n",
    "                best_rmse = rmse\n",
    "                best_k = k\n",
    "        return best_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "\n",
    "    def __init__(self, X_train, Y_train):\n",
    "        \"\"\"\n",
    "        :param X_train: training set\n",
    "        :param Y_train: outputs of the training set\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        n = self.X_train.shape[1]\n",
    "        self.w = np.random.normal(0, 0.5, size=(n, 1))\n",
    "        self.b = np.random.normal(0, 0.5)\n",
    "\n",
    "    def train(self, iterations, learning_rate, print_iterations=False):\n",
    "        \"\"\"\n",
    "        Optimizes the objective function with the gradient descent algorithm\n",
    "        in order to find well suited parameters w and b.\n",
    "        :param iterations: The number of gradient descent iterations\n",
    "        :param learning_rate: Step size of the gradient descent algorithm\n",
    "        :param print_iterations: Whether or not the current iteration and its training loss shall be printed to console\n",
    "        \"\"\"\n",
    "        cost_prev = np.inf\n",
    "        current_learning_rate = learning_rate\n",
    "        for _ in range(iterations):\n",
    "            prediction = np.dot(self.X_train, self.w) + self.b\n",
    "            cost = self._cost(prediction)\n",
    "            if print_iterations:\n",
    "                print('Iteration ' + str(_) + ', cost: ' + str(cost))\n",
    "            self._update_weights(learning_rate, prediction)\n",
    "\n",
    "            if cost >= cost_prev:\n",
    "                current_learning_rate = current_learning_rate * 0.5\n",
    "                continue\n",
    "\n",
    "            if np.fabs(cost - cost_prev) < 1e-12:\n",
    "                print('Stopping early')\n",
    "                break\n",
    "\n",
    "            cost_prev = cost\n",
    "\n",
    "    def test(self, X_test, Y_test):\n",
    "        \"\"\"\n",
    "        Test the current parameters on the provided test set.\n",
    "        :param X_test: Test set\n",
    "        :param Y_test: Output values for the test set\n",
    "        :return: RMSE for the test set, predictions on the test set\n",
    "        \"\"\"\n",
    "        prediction = np.dot(X_test, self.w) + self.b\n",
    "        rmse = np.sqrt(np.mean(np.square(prediction - Y_test)))\n",
    "        return rmse, prediction\n",
    "\n",
    "    def _update_weights(self, learning_rate, prediction):\n",
    "        \"\"\"\n",
    "        Implements the parameter update in the direction of the negative\n",
    "        gradient of the objective function.\n",
    "        :param learning_rate: The step size\n",
    "        :param prediction: The predictions from the current iteration\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        grad_w, grad_b = self._compute_gradient(prediction)\n",
    "        self.w = self.w - learning_rate * grad_w\n",
    "        self.b = self.b - learning_rate * grad_b\n",
    "\n",
    "    def _compute_gradient(self, prediction):\n",
    "        \"\"\"\n",
    "        Computes the gradient for the parameters w and b.\n",
    "        :param prediction: The predictions from the current iteration\n",
    "        :return: gradient of w, gradient of b\n",
    "        \"\"\"\n",
    "        m = prediction.shape[0]\n",
    "        grad_w = np.dot(self.X_train.T, (prediction - self.Y_train)) / m\n",
    "        grad_b = np.sum((prediction - self.Y_train)) / m\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def _cost(self, prediction):\n",
    "        \"\"\"\n",
    "        Computes the cost for the current set of parameters.\n",
    "        :param prediction: The predictions from the current iteration\n",
    "        :return: Cost value\n",
    "        \"\"\"\n",
    "        return np.sum((prediction - self.Y_train)**2) / (2 * self.Y_train.shape[0])\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Resets the parameters of the model.\n",
    "        \"\"\"\n",
    "        n = self.X_train.shape[1]\n",
    "        self.w = np.random.normal(0, 0.5, size=(n, 1))\n",
    "        self.b = np.random.normal(0, 0.5)\n",
    "\n",
    "    def normal_equation(self, X_test, Y_test):\n",
    "        \"\"\"\n",
    "        Computes the exact solution for the linear regression problem via the normal equation.\n",
    "        :param X_test: Test set\n",
    "        :param Y_test: Output values of the test set\n",
    "        :return: RMSE of the test set, predictions for the test set\n",
    "        \"\"\"\n",
    "        X = np.copy(self.X_train)\n",
    "        X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "        w = np.dot(np.dot(np.dot(X.T, X), X.T), self.Y_train)\n",
    "        X_ = np.copy(X_test)\n",
    "        X_ = np.append(np.ones((X_.shape[0],1)), X_, axis=1)\n",
    "        print(w.shape)\n",
    "        print(X_.shape)\n",
    "        prediction = np.dot(X_, w)\n",
    "        rmse = np.sqrt(np.mean(np.square(prediction - Y_test)))\n",
    "        return rmse, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    def __init__(self, is_leaf, mean, std, feature, split, left, right):\n",
    "        \"\"\"\n",
    "        :param is_leaf: Whether or not it is a leaf node\n",
    "        :param mean: The current mean of all the examples contained in the node\n",
    "        :param std: The current standard deviation of all the examples contained in the node\n",
    "        :param feature: The feature by which the split is performed\n",
    "        :param split: The actual threshold value for the split\n",
    "        :param left: Left child node\n",
    "        :param right: Right child node\n",
    "        \"\"\"\n",
    "        self.is_leaf = is_leaf\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.feature = feature\n",
    "        self.split = split\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, X_train, Y_train, min_nodes):\n",
    "        \"\"\"\n",
    "        :param X_train: Training set\n",
    "        :param Y_train: Output values for the training set\n",
    "        :param min_nodes: If a node contains equal or less than 'min_nodes',\n",
    "                            then the recursion is terminated and a leaf node is created\n",
    "        \"\"\"\n",
    "        self.root = self._create_tree(X_train, Y_train, min_nodes)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the output value of the input vector.\n",
    "        :param x: Input vector\n",
    "        :return: Mean of the leaf node, standard deviation of the leaf node\n",
    "        \"\"\"\n",
    "        return self._predict(self.root, x)\n",
    "\n",
    "    def predict_all(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the output values of an entire set of input vectors.\n",
    "        :param X: Set of input vectors\n",
    "        :return: Mean of the leaf nodes, standard deviation of the leaf nodes\n",
    "        \"\"\"\n",
    "        means = np.zeros((X.shape[0], 1))\n",
    "        stds = np.zeros((X.shape[0], 1))\n",
    "        for i in range(X.shape[0]):\n",
    "            mean, std = self._predict(self.root, X[i])\n",
    "            means[i] = mean\n",
    "            stds[i] = std\n",
    "        return means, stds\n",
    "\n",
    "    def _predict(self, node, x):\n",
    "        \"\"\"\n",
    "        Helper function for the predict function.\n",
    "        :param node: Current node in the tree\n",
    "        :param x: Input vector for which we predict the output value\n",
    "        :return: Mean of the leaf node, standard deviation of the leaf node\n",
    "        \"\"\"\n",
    "        if node.is_leaf:\n",
    "            return node.mean, node.std\n",
    "        if x[node.feature] <= node.split:\n",
    "            return self._predict(node.left, x)\n",
    "        else:\n",
    "            return self._predict(node.right, x)\n",
    "\n",
    "    def _create_tree(self, X, Y, min_nodes):\n",
    "        \"\"\"\n",
    "        Recursively creates a decision tree.\n",
    "        :param X: Current set of training examples\n",
    "        :param Y: Output values for the training examples\n",
    "        :param min_nodes: If a node contains equal or less than 'min_nodes', then\n",
    "                            the recursion is terminated and a leaf node is created\n",
    "        :return: Eventually returns the root of the decision tree\n",
    "        \"\"\"\n",
    "        if len(X) <= min_nodes:\n",
    "            return Node(is_leaf=True, mean=np.mean(Y), std=np.std(Y), feature=None, split=None, left=None, right=None)\n",
    "        feature, best_split, left_indices, right_indices = DecisionTree._best_split_random(X, Y)\n",
    "        left = self._create_tree(X[left_indices], Y[left_indices], min_nodes)\n",
    "        right = self._create_tree(X[right_indices], Y[right_indices], min_nodes)\n",
    "        return Node(is_leaf=False, mean=np.mean(Y), std=np.std(Y), feature=feature, split=best_split, left=left, right=right)\n",
    "\n",
    "    @staticmethod\n",
    "    def _best_split_random(X, Y):\n",
    "        \"\"\"\n",
    "        Computes the best split for a given set of training examples.\n",
    "        Tries out only a random subset (of size num_features / 2 + 1) of the features.\n",
    "        Tries out only a random subset (of size sqrt(num_values)*2) of all of the existing values for a specific feature.\n",
    "        :param X: Set of training examples\n",
    "        :param Y: Output values of training examples\n",
    "        :return: Feature for split, threshold value for split, indices of left child, indices of right child\n",
    "        \"\"\"\n",
    "        min_sse = np.inf\n",
    "        feature = None\n",
    "        best_split = None\n",
    "        left = None\n",
    "        right = None\n",
    "        random_feature_indices_ = np.random.choice(np.arange(X.shape[1]), size=int(X.shape[1] // 2 + 1), replace=False)\n",
    "        for j in range(len(random_feature_indices_)):\n",
    "            i = random_feature_indices_[j]\n",
    "            num_values = X[:, i].shape[0]\n",
    "            random_values_indices = np.random.choice(np.arange(num_values), size=int(2 * np.sqrt(num_values)), replace=False)\n",
    "            values = X[random_values_indices, i]\n",
    "            for split in values:\n",
    "                left_indices = X[:, i:(i+1)] <= split\n",
    "                left_indices, _ = np.nonzero(left_indices)\n",
    "                right_indices = X[:, i:(i+1)] > split\n",
    "                right_indices, _ = np.nonzero(right_indices)\n",
    "                sse = DecisionTree._sum_of_squared_errors(Y[left_indices], Y[right_indices])\n",
    "                if sse < min_sse:\n",
    "                    feature = i\n",
    "                    best_split = split\n",
    "                    left = left_indices\n",
    "                    right = right_indices\n",
    "                    min_sse = sse\n",
    "        return feature, best_split, left, right\n",
    "\n",
    "    @staticmethod\n",
    "    def _best_split(X, Y):\n",
    "        \"\"\"\n",
    "        Computes the best split for a given set of training examples.\n",
    "        Tries out all of the existing values for a specific feature.\n",
    "        :param X: Set of training examples\n",
    "        :param Y: Output values of training examples\n",
    "        :return: Feature for split, threshold value for split, indices of left child, indices of right child\n",
    "        \"\"\"\n",
    "        min_sse = np.inf\n",
    "        feature = None\n",
    "        best_split = None\n",
    "        left = None\n",
    "        right = None\n",
    "        for i in range(X.shape[1]):\n",
    "            values = np.unique(X[:, i])\n",
    "            for split in values:\n",
    "                left_indices = X[:, i:(i+1)] <= split\n",
    "                left_indices, _ = np.nonzero(left_indices)\n",
    "                right_indices = X[:, i:(i+1)] > split\n",
    "                right_indices, _ = np.nonzero(right_indices)\n",
    "                sse = DecisionTree._sum_of_squared_errors(Y[left_indices], Y[right_indices])\n",
    "                if sse < min_sse:\n",
    "                    feature = i\n",
    "                    best_split = split\n",
    "                    left = left_indices\n",
    "                    right = right_indices\n",
    "                    min_sse = sse\n",
    "        return feature, best_split, left, right\n",
    "\n",
    "    @staticmethod\n",
    "    def _sum_of_squared_errors(Y1, Y2):\n",
    "        \"\"\"\n",
    "        Computes the sum of squared errors between two vectors.\n",
    "        :param Y1: First vector\n",
    "        :param Y2: Second vector\n",
    "        :return: Sum of squared errors\n",
    "        \"\"\"\n",
    "        if len(Y1) == 0:\n",
    "            sse1 = 0\n",
    "        else:\n",
    "            sse1 = np.sum((Y1 - np.mean(Y1))**2)\n",
    "        if len(Y2) == 0:\n",
    "            sse2 = 0\n",
    "        else:\n",
    "            sse2 = np.sum((Y2 - np.mean(Y2)) ** 2)\n",
    "        return sse1 + sse2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "\n",
    "    def __init__(self, X_train, Y_train, num_trees, training_set_size, min_nodes):\n",
    "        \"\"\"\n",
    "        :param X_train: Training set\n",
    "        :param Y_train: Output values for the training set\n",
    "        :param num_trees: Number of decision trees\n",
    "        :param training_set_size: Size of the training set that each decision tree is build with\n",
    "        :param min_nodes: If a node contains equal or less than 'min_nodes',\n",
    "                            then the recursion is terminated and a leaf node is created\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.tree_list = []\n",
    "        self._create_forest(num_trees, training_set_size, min_nodes)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the output value of the input vector using\n",
    "        the average over all of the prediction from the decision trees.\n",
    "        :param x: Input vector\n",
    "        :return: Mean of the leaf nodes, standard deviation of the leaf nodes\n",
    "        \"\"\"\n",
    "        mean_all = 0\n",
    "        std_all = 0\n",
    "        for tree in self.tree_list:\n",
    "            mean, std = tree.predict(x)\n",
    "            mean_all = mean_all + mean\n",
    "            std_all = std_all + std\n",
    "        mean_all = mean_all / len(self.tree_list)\n",
    "        std_all = std_all / len(self.tree_list)\n",
    "        return mean_all, std_all\n",
    "\n",
    "    def predict_all(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the output values of an entire set of input vectors\n",
    "        using the average over all of the prediction from the decision trees.\n",
    "        :param X: Set of input vectors\n",
    "        :return: Mean of the leaf nodes, standard deviation of the leaf nodes\n",
    "        \"\"\"\n",
    "        means = np.zeros((X.shape[0], 1))\n",
    "        stds = np.zeros((X.shape[0], 1))\n",
    "        for i in range(X.shape[0]):\n",
    "            mean, std = self.predict(X[i])\n",
    "            means[i] = mean\n",
    "            stds[i] = std\n",
    "        return means, stds\n",
    "\n",
    "    def _create_forest(self, num_trees, training_set_size, min_nodes):\n",
    "        \"\"\"\n",
    "        Creates a random forest.\n",
    "        :param num_trees: Number of decision trees\n",
    "        :param training_set_size: Size of the training set that each decision tree is build with\n",
    "        :param min_nodes: If a node contains equal or less than 'min_nodes',\n",
    "                            then the recursion is terminated and a leaf node is created\n",
    "        \"\"\"\n",
    "        for i in range(num_trees):\n",
    "            tree = self._create_tree(training_set_size, min_nodes)\n",
    "            self.tree_list.append(tree)\n",
    "\n",
    "    def _create_tree(self, training_set_size, min_nodes):\n",
    "        \"\"\"\n",
    "        Creates a decision tree from a random subset of the training examples.\n",
    "        :param training_set_size: Size of the training set that each decision tree is build with\n",
    "        :param min_nodes: If a node contains equal or less than 'min_nodes',\n",
    "                            then the recursion is terminated and a leaf node is created\n",
    "        :return: Decision tree\n",
    "        \"\"\"\n",
    "        random_subset_indices = np.random.choice(np.arange(self.X_train.shape[0]), size=training_set_size, replace=False)\n",
    "        X = self.X_train[random_subset_indices]\n",
    "        Y = self.Y_train[random_subset_indices]\n",
    "        tree = DecisionTree(X, Y, min_nodes=min_nodes)\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcess:\n",
    "\n",
    "    def __init__(self, X_train, Y_train):\n",
    "        \"\"\"\n",
    "        :param X_train: training set\n",
    "        :param Y_train: outputs of the training set\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "\n",
    "    def draw_from_prior(self, X_test, num_samples, length_scale):\n",
    "        \"\"\"\n",
    "        Returns samples drawn from the prior distribution at the test inputs.\n",
    "        :param X_test: test set\n",
    "        :param num_samples: number of samples to be drawn\n",
    "        :param length_scale: determines the smoothness of the function\n",
    "        :return: samples\n",
    "        \"\"\"\n",
    "        K__star_star = GaussianProcess.squared_exponential_kernel(X_test, X_test, length_scale)\n",
    "        return np.dot(K__star_star, np.random.normal(size=(len(X_test), num_samples)))\n",
    "\n",
    "    def draw_from_posterior(self, X_test, num_samples, length_scale):\n",
    "        \"\"\"\n",
    "        Returns samples drawn from the posterior at the test inputs.\n",
    "        :param X_test: test set\n",
    "        :param num_samples: number of samples to be drawn\n",
    "        :param length_scale: determines the smoothness of the function\n",
    "        :return: samples\n",
    "        \"\"\"\n",
    "        K = GaussianProcess.squared_exponential_kernel(self.X_train, self.X_train, length_scale)\n",
    "        K_inverse = np.linalg.inv(K)\n",
    "        K__star_star = GaussianProcess.squared_exponential_kernel(X_test, X_test, length_scale)\n",
    "        K_star_ = GaussianProcess.squared_exponential_kernel(X_test, self.X_train, length_scale)\n",
    "        K__star = GaussianProcess.squared_exponential_kernel(self.X_train, X_test, length_scale)\n",
    "        K_conditioned = K__star_star - np.dot(np.dot(K_star_, K_inverse), K__star)\n",
    "        mu = np.dot(np.dot(K_star_, K_inverse), self.Y_train)\n",
    "        return mu + np.dot(K_conditioned, np.random.normal(size=(len(X_test), num_samples)))\n",
    "\n",
    "    def predict(self, X_test, length_scale):\n",
    "        \"\"\"\n",
    "        Predicts the output values for the provided inputs.\n",
    "        :param X_test: Test set\n",
    "        :param length_scale: determines the smoothness of the function\n",
    "        :return: predictions, variance for the predictions\n",
    "        \"\"\"\n",
    "        K = GaussianProcess.squared_exponential_kernel(self.X_train, self.X_train, length_scale)\n",
    "        K_inverse = np.linalg.inv(K)\n",
    "        K__star_star = GaussianProcess.squared_exponential_kernel(X_test, X_test, length_scale)\n",
    "        K_star_ = GaussianProcess.squared_exponential_kernel(X_test, self.X_train, length_scale)\n",
    "        K__star = GaussianProcess.squared_exponential_kernel(self.X_train, X_test, length_scale)\n",
    "        K_conditioned = K__star_star - np.dot(np.dot(K_star_, K_inverse), K__star)\n",
    "        sigma_squared = np.diag(K_conditioned)\n",
    "        mu = np.dot(np.dot(K_star_, K_inverse), self.Y_train)\n",
    "        return mu, sigma_squared\n",
    "\n",
    "    def predict_cholesky(self, X_test, length_scale):\n",
    "        \"\"\"\n",
    "        Predicts the output values for the provided inputs. Uses the Cholesky decomposition (more on that in the report)\n",
    "        in order to reduce computation and improve numerical stability.\n",
    "        :param X_test: Test set\n",
    "        :param length_scale: determines the smoothness of the function\n",
    "        :return: predictions, variance for the predictions\n",
    "        \"\"\"\n",
    "        K = GaussianProcess.squared_exponential_kernel(self.X_train, self.X_train, length_scale)\n",
    "        L = np.linalg.cholesky(K)\n",
    "        K_star_ = GaussianProcess.squared_exponential_kernel(X_test, self.X_train, length_scale)\n",
    "        v = np.linalg.solve(L, self.Y_train)\n",
    "        w = np.linalg.solve(L.T, v)\n",
    "        mu = np.dot(K_star_, w)\n",
    "        q = np.linalg.solve(L, K_star_.T)\n",
    "        z = np.linalg.solve(L.T, q)\n",
    "        K__star_star = GaussianProcess.squared_exponential_kernel(X_test, X_test, length_scale)\n",
    "        K_conditioned = K__star_star - np.dot(K_star_, z)\n",
    "        sigma_squared = np.diag(K_conditioned)\n",
    "        return mu, sigma_squared\n",
    "\n",
    "    def test(self, X_test, Y_test, length_scale, use_cholesky):\n",
    "        \"\"\"\n",
    "        Tests the algorithm on the provided test set.\n",
    "        :param X_test: test set\n",
    "        :param Y_test: output values of the test set\n",
    "        :param length_scale: determines the smoothness of the function\n",
    "        :param use_cholesky: True: use Cholesky decomposition for computation, False: do not\n",
    "        :return: RMSE for the test set, predictions on the test set, variance for the predictions\n",
    "        \"\"\"\n",
    "        if use_cholesky:\n",
    "            mu, sigma = self.predict_cholesky(X_test, length_scale)\n",
    "        else:\n",
    "            mu, sigma = self.predict(X_test, length_scale)\n",
    "        rmse = np.sqrt(np.mean(np.square(mu - Y_test)))\n",
    "        return rmse, mu, sigma\n",
    "\n",
    "    @staticmethod\n",
    "    def squared_exponential_kernel(x1, x2, length_scale):\n",
    "        \"\"\"\n",
    "        Returns the covariance matrix according to the squared exponential kernel between x1 and x2.\n",
    "        :param x1: First set of input vectors\n",
    "        :param x2: Second set of input vectors\n",
    "        :param length_scale: Determines the smoothness of the function\n",
    "        :return: Covariance matrix\n",
    "        \"\"\"\n",
    "        dist = np.sum(x1**2, axis=1, keepdims=True) + np.sum(x2**2, axis=1) - 2 * np.dot(x1, x2.T)\n",
    "        return np.exp(-(1 / (2 * length_scale**2)) * dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors, sarcos:\n",
      "\tThis will take about 30 seconds, please wait...\n",
      "\tRMSE: 3.3386142376528443\n",
      "Linear regression, sarcos:\n",
      "\tThis will take about 5 seconds, please wait...\n",
      "Stopping early\n",
      "\tRMSE: 5.460017555852863\n",
      "Regression forests, sarcos:\n",
      "\tThis will take about 6 minutes, please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tRMSE: 4.7163346809985445\n",
      "Gaussian processes, sarcos:\n",
      "\tThis will take about 15 minutes and use a lot of memory, please wait...\n",
      "\tRMSE: 5.438570800663122\n"
     ]
    }
   ],
   "source": [
    "data = DataLoader('sarcos_inv.csv')\n",
    "\n",
    "def sarcos_nearest_neighbors():\n",
    "    X_train, Y_train, X_cv, Y_cv, X_test, Y_test = data.get_data_split([0.6, 0.2, 0.2], normalize=True)\n",
    "    print('Nearest neighbors, sarcos:')\n",
    "    print('\\tThis will take about 30 seconds, please wait...')\n",
    "    model = NearestNeighbors(X_train, Y_train)\n",
    "    #k = model.get_best_k(X_cv, Y_cv, 10)\n",
    "    rmse, _ = model.test(X_test, Y_test, 4)\n",
    "    print('\\tRMSE: ' + str(rmse))\n",
    "\n",
    "\n",
    "def sarcos_linear_regression():\n",
    "    X_train, Y_train, X_cv, Y_cv, X_test, Y_test = data.get_data_split([0.6, 0.2, 0.2], normalize=True)\n",
    "    print('Linear regression, sarcos:')\n",
    "    print('\\tThis will take about 5 seconds, please wait...')\n",
    "    model = LinearRegression(X_train, Y_train)\n",
    "    model.train(10000, 0.1, print_iterations=False)\n",
    "    rmse, prediction = model.test(X_test, Y_test)\n",
    "    rmse_train, _ = model.test(X_train, Y_train)\n",
    "    print('\\tRMSE: ' + str(rmse))\n",
    "\n",
    "\n",
    "def sarcos_regression_forest():\n",
    "    X_train, Y_train, X_cv, Y_cv, X_test, Y_test = data.get_data_split([0.6, 0.2, 0.2], normalize=False)\n",
    "    print('Regression forests, sarcos:')\n",
    "    print('\\tThis will take about 6 minutes, please wait...')\n",
    "    model = RandomForest(X_train, Y_train, num_trees=30, training_set_size=4000, min_nodes=5)\n",
    "    mean, std = model.predict_all(X_test)\n",
    "    rmse = np.sqrt(np.mean(np.square(mean - Y_test)))\n",
    "    print('\\tRMSE: ' + str(rmse))\n",
    "\n",
    "\n",
    "def sarcos_gaussian_processes():\n",
    "    X_train, Y_train, X_cv, Y_cv, X_test, Y_test = data.get_data_split([0.6, 0.2, 0.2], normalize=True)\n",
    "    print('Gaussian processes, sarcos:')\n",
    "    print('\\tThis will take about 15 minutes and use a lot of memory, please wait...')\n",
    "    model = GaussianProcess(X_train, Y_train)\n",
    "    rmse, mean, var = model.test(X_test, Y_test, length_scale=0.9, use_cholesky=True)\n",
    "    print('\\tRMSE: ' + str(rmse))\n",
    "\n",
    "\n",
    "sarcos_nearest_neighbors()\n",
    "sarcos_linear_regression()\n",
    "sarcos_regression_forest()\n",
    "sarcos_gaussian_processes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
