{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "\n",
    "    def __init__(self, is_leaf, mean, std, feature, split, left, right):\n",
    "        \"\"\"\n",
    "        :param is_leaf: Whether or not it is a leaf node\n",
    "        :param mean: The current mean of all the examples contained in the node\n",
    "        :param std: The current standard deviation of all the examples contained in the node\n",
    "        :param feature: The feature by which the split is performed\n",
    "        :param split: The actual threshold value for the split\n",
    "        :param left: Left child node\n",
    "        :param right: Right child node\n",
    "        \"\"\"\n",
    "        self.is_leaf = is_leaf\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.feature = feature\n",
    "        self.split = split\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, X_train, Y_train, min_nodes):\n",
    "        \"\"\"\n",
    "        :param X_train: Training set\n",
    "        :param Y_train: Output values for the training set\n",
    "        :param min_nodes: If a node contains equal or less than 'min_nodes',\n",
    "                            then the recursion is terminated and a leaf node is created\n",
    "        \"\"\"\n",
    "        self.root = self._create_tree(X_train, Y_train, min_nodes)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the output value of the input vector.\n",
    "        :param x: Input vector\n",
    "        :return: Mean of the leaf node, standard deviation of the leaf node\n",
    "        \"\"\"\n",
    "        return self._predict(self.root, x)\n",
    "\n",
    "    def predict_all(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the output values of an entire set of input vectors.\n",
    "        :param X: Set of input vectors\n",
    "        :return: Mean of the leaf nodes, standard deviation of the leaf nodes\n",
    "        \"\"\"\n",
    "        means = np.zeros((X.shape[0], 1))\n",
    "        stds = np.zeros((X.shape[0], 1))\n",
    "        for i in range(X.shape[0]):\n",
    "            mean, std = self._predict(self.root, X[i])\n",
    "            means[i] = mean\n",
    "            stds[i] = std\n",
    "        return means, stds\n",
    "\n",
    "    def _predict(self, node, x):\n",
    "        \"\"\"\n",
    "        Helper function for the predict function.\n",
    "        :param node: Current node in the tree\n",
    "        :param x: Input vector for which we predict the output value\n",
    "        :return: Mean of the leaf node, standard deviation of the leaf node\n",
    "        \"\"\"\n",
    "        if node.is_leaf:\n",
    "            return node.mean, node.std\n",
    "        if x[node.feature] <= node.split:\n",
    "            return self._predict(node.left, x)\n",
    "        else:\n",
    "            return self._predict(node.right, x)\n",
    "\n",
    "    def _create_tree(self, X, Y, min_nodes):\n",
    "        \"\"\"\n",
    "        Recursively creates a decision tree.\n",
    "        :param X: Current set of training examples\n",
    "        :param Y: Output values for the training examples\n",
    "        :param min_nodes: If a node contains equal or less than 'min_nodes', then\n",
    "                            the recursion is terminated and a leaf node is created\n",
    "        :return: Eventually returns the root of the decision tree\n",
    "        \"\"\"\n",
    "        if len(X) <= min_nodes:\n",
    "            return Node(is_leaf=True, mean=np.mean(Y), std=np.std(Y), feature=None, split=None, left=None, right=None)\n",
    "        feature, best_split, left_indices, right_indices = DecisionTree._best_split_random(X, Y)\n",
    "        left = self._create_tree(X[left_indices], Y[left_indices], min_nodes)\n",
    "        right = self._create_tree(X[right_indices], Y[right_indices], min_nodes)\n",
    "        return Node(is_leaf=False, mean=np.mean(Y), std=np.std(Y), feature=feature, split=best_split, left=left, right=right)\n",
    "\n",
    "    @staticmethod\n",
    "    def _best_split_random(X, Y):\n",
    "        \"\"\"\n",
    "        Computes the best split for a given set of training examples.\n",
    "        Tries out only a random subset (of size num_features / 2 + 1) of the features.\n",
    "        Tries out only a random subset (of size sqrt(num_values)*2) of all of the existing values for a specific feature.\n",
    "        :param X: Set of training examples\n",
    "        :param Y: Output values of training examples\n",
    "        :return: Feature for split, threshold value for split, indices of left child, indices of right child\n",
    "        \"\"\"\n",
    "        min_sse = np.inf\n",
    "        feature = None\n",
    "        best_split = None\n",
    "        left = None\n",
    "        right = None\n",
    "        random_feature_indices_ = np.random.choice(np.arange(X.shape[1]), size=int(X.shape[1] // 2 + 1), replace=False)\n",
    "        for j in range(len(random_feature_indices_)):\n",
    "            i = random_feature_indices_[j]\n",
    "            num_values = X[:, i].shape[0]\n",
    "            random_values_indices = np.random.choice(np.arange(num_values), size=int(2 * np.sqrt(num_values)), replace=False)\n",
    "            values = X[random_values_indices, i]\n",
    "            for split in values:\n",
    "                left_indices = X[:, i:(i+1)] <= split\n",
    "                left_indices, _ = np.nonzero(left_indices)\n",
    "                right_indices = X[:, i:(i+1)] > split\n",
    "                right_indices, _ = np.nonzero(right_indices)\n",
    "                sse = DecisionTree._sum_of_squared_errors(Y[left_indices], Y[right_indices])\n",
    "                if sse < min_sse:\n",
    "                    feature = i\n",
    "                    best_split = split\n",
    "                    left = left_indices\n",
    "                    right = right_indices\n",
    "                    min_sse = sse\n",
    "        return feature, best_split, left, right\n",
    "\n",
    "    @staticmethod\n",
    "    def _best_split(X, Y):\n",
    "        \"\"\"\n",
    "        Computes the best split for a given set of training examples.\n",
    "        Tries out all of the existing values for a specific feature.\n",
    "        :param X: Set of training examples\n",
    "        :param Y: Output values of training examples\n",
    "        :return: Feature for split, threshold value for split, indices of left child, indices of right child\n",
    "        \"\"\"\n",
    "        min_sse = np.inf\n",
    "        feature = None\n",
    "        best_split = None\n",
    "        left = None\n",
    "        right = None\n",
    "        for i in range(X.shape[1]):\n",
    "            values = np.unique(X[:, i])\n",
    "            for split in values:\n",
    "                left_indices = X[:, i:(i+1)] <= split\n",
    "                left_indices, _ = np.nonzero(left_indices)\n",
    "                right_indices = X[:, i:(i+1)] > split\n",
    "                right_indices, _ = np.nonzero(right_indices)\n",
    "                sse = DecisionTree._sum_of_squared_errors(Y[left_indices], Y[right_indices])\n",
    "                if sse < min_sse:\n",
    "                    feature = i\n",
    "                    best_split = split\n",
    "                    left = left_indices\n",
    "                    right = right_indices\n",
    "                    min_sse = sse\n",
    "        return feature, best_split, left, right\n",
    "\n",
    "    @staticmethod\n",
    "    def _sum_of_squared_errors(Y1, Y2):\n",
    "        \"\"\"\n",
    "        Computes the sum of squared errors between two vectors.\n",
    "        :param Y1: First vector\n",
    "        :param Y2: Second vector\n",
    "        :return: Sum of squared errors\n",
    "        \"\"\"\n",
    "        if len(Y1) == 0:\n",
    "            sse1 = 0\n",
    "        else:\n",
    "            sse1 = np.sum((Y1 - np.mean(Y1))**2)\n",
    "        if len(Y2) == 0:\n",
    "            sse2 = 0\n",
    "        else:\n",
    "            sse2 = np.sum((Y2 - np.mean(Y2)) ** 2)\n",
    "        return sse1 + sse2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
