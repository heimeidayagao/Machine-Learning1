{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class GaussianProcess:\n",
    "\n",
    "    def __init__(self, X_train, Y_train):\n",
    "        \"\"\"\n",
    "        :param X_train: training set\n",
    "        :param Y_train: outputs of the training set\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "\n",
    "    def draw_from_prior(self, X_test, num_samples, length_scale):\n",
    "        \"\"\"\n",
    "        Returns samples drawn from the prior distribution at the test inputs.\n",
    "        :param X_test: test set\n",
    "        :param num_samples: number of samples to be drawn\n",
    "        :param length_scale: determines the smoothness of the function\n",
    "        :return: samples\n",
    "        \"\"\"\n",
    "        K__star_star = GaussianProcess.squared_exponential_kernel(X_test, X_test, length_scale)\n",
    "        return np.dot(K__star_star, np.random.normal(size=(len(X_test), num_samples)))\n",
    "\n",
    "    def draw_from_posterior(self, X_test, num_samples, length_scale):\n",
    "        \"\"\"\n",
    "        Returns samples drawn from the posterior at the test inputs.\n",
    "        :param X_test: test set\n",
    "        :param num_samples: number of samples to be drawn\n",
    "        :param length_scale: determines the smoothness of the function\n",
    "        :return: samples\n",
    "        \"\"\"\n",
    "        K = GaussianProcess.squared_exponential_kernel(self.X_train, self.X_train, length_scale)\n",
    "        K_inverse = np.linalg.inv(K)\n",
    "        K__star_star = GaussianProcess.squared_exponential_kernel(X_test, X_test, length_scale)\n",
    "        K_star_ = GaussianProcess.squared_exponential_kernel(X_test, self.X_train, length_scale)\n",
    "        K__star = GaussianProcess.squared_exponential_kernel(self.X_train, X_test, length_scale)\n",
    "        K_conditioned = K__star_star - np.dot(np.dot(K_star_, K_inverse), K__star)\n",
    "        mu = np.dot(np.dot(K_star_, K_inverse), self.Y_train)\n",
    "        return mu + np.dot(K_conditioned, np.random.normal(size=(len(X_test), num_samples)))\n",
    "\n",
    "    def predict(self, X_test, length_scale):\n",
    "        \"\"\"\n",
    "        Predicts the output values for the provided inputs.\n",
    "        :param X_test: Test set\n",
    "        :param length_scale: determines the smoothness of the function\n",
    "        :return: predictions, variance for the predictions\n",
    "        \"\"\"\n",
    "        K = GaussianProcess.squared_exponential_kernel(self.X_train, self.X_train, length_scale)\n",
    "        K_inverse = np.linalg.inv(K)\n",
    "        K__star_star = GaussianProcess.squared_exponential_kernel(X_test, X_test, length_scale)\n",
    "        K_star_ = GaussianProcess.squared_exponential_kernel(X_test, self.X_train, length_scale)\n",
    "        K__star = GaussianProcess.squared_exponential_kernel(self.X_train, X_test, length_scale)\n",
    "        K_conditioned = K__star_star - np.dot(np.dot(K_star_, K_inverse), K__star)\n",
    "        sigma_squared = np.diag(K_conditioned)\n",
    "        mu = np.dot(np.dot(K_star_, K_inverse), self.Y_train)\n",
    "        return mu, sigma_squared\n",
    "\n",
    "    def predict_cholesky(self, X_test, length_scale):\n",
    "        \"\"\"\n",
    "        Predicts the output values for the provided inputs. Uses the Cholesky decomposition (more on that in the report)\n",
    "        in order to reduce computation and improve numerical stability.\n",
    "        :param X_test: Test set\n",
    "        :param length_scale: determines the smoothness of the function\n",
    "        :return: predictions, variance for the predictions\n",
    "        \"\"\"\n",
    "        K = GaussianProcess.squared_exponential_kernel(self.X_train, self.X_train, length_scale)\n",
    "        L = np.linalg.cholesky(K)\n",
    "        K_star_ = GaussianProcess.squared_exponential_kernel(X_test, self.X_train, length_scale)\n",
    "        v = np.linalg.solve(L, self.Y_train)\n",
    "        w = np.linalg.solve(L.T, v)\n",
    "        mu = np.dot(K_star_, w)\n",
    "        q = np.linalg.solve(L, K_star_.T)\n",
    "        z = np.linalg.solve(L.T, q)\n",
    "        K__star_star = GaussianProcess.squared_exponential_kernel(X_test, X_test, length_scale)\n",
    "        K_conditioned = K__star_star - np.dot(K_star_, z)\n",
    "        sigma_squared = np.diag(K_conditioned)\n",
    "        return mu, sigma_squared\n",
    "\n",
    "    def test(self, X_test, Y_test, length_scale, use_cholesky):\n",
    "        \"\"\"\n",
    "        Tests the algorithm on the provided test set.\n",
    "        :param X_test: test set\n",
    "        :param Y_test: output values of the test set\n",
    "        :param length_scale: determines the smoothness of the function\n",
    "        :param use_cholesky: True: use Cholesky decomposition for computation, False: do not\n",
    "        :return: RMSE for the test set, predictions on the test set, variance for the predictions\n",
    "        \"\"\"\n",
    "        if use_cholesky:\n",
    "            mu, sigma = self.predict_cholesky(X_test, length_scale)\n",
    "        else:\n",
    "            mu, sigma = self.predict(X_test, length_scale)\n",
    "        rmse = np.sqrt(np.mean(np.square(mu - Y_test)))\n",
    "        return rmse, mu, sigma\n",
    "\n",
    "    @staticmethod\n",
    "    def squared_exponential_kernel(x1, x2, length_scale):\n",
    "        \"\"\"\n",
    "        Returns the covariance matrix according to the squared exponential kernel between x1 and x2.\n",
    "        :param x1: First set of input vectors\n",
    "        :param x2: Second set of input vectors\n",
    "        :param length_scale: Determines the smoothness of the function\n",
    "        :return: Covariance matrix\n",
    "        \"\"\"\n",
    "        dist = np.sum(x1**2, axis=1, keepdims=True) + np.sum(x2**2, axis=1) - 2 * np.dot(x1, x2.T)\n",
    "        return np.exp(-(1 / (2 * length_scale**2)) * dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
